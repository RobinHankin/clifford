---
title: "Dual numbers implemented using Clifford algebra, and a case-study of automatic differentiation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("clifford")
```

<p style="text-align: right;">
![](`r system.file("help/figures/clifford.png", package = "clifford")`){width=10%}
</p>

(takes about twenty minutes to run without cache)

*NOTE* This vignette shows a use-case for dual numbers, as applied to
 automatic differentiation.  If you want to actually use AD, I would
 strongly recommend you consider the excellent `dual` package (Sartore
 2022), which offers a nice user interface, efficient coding, and lots
 of examples.

## Dual numbers

Dual numbers are expressions of the form $a+b\epsilon$, with
$a,b\in\mathcal{R}$ and $\epsilon^2=0$.  If we define

$$\begin{eqnarray}
(a+b\epsilon) + (c+d\epsilon) &=& (a+c) + (b+d)\epsilon\\
(a+b\epsilon)(c+d\epsilon) &=& ac + (ad+bc)\epsilon
\end{eqnarray}$$


then we recover distributivity and associativity.  The Clifford
representation is easy, first setting the signature to zero:

```{r defineeps}
library(clifford)
signature(0)
options(maxdim = 1) # safety measure
eps <- clifford(list(1))  # 'eps'  is shorthand for epsilon
eps   # regular Clifford object
drop(eps^2)  # check that eps^2==0
```

Then, to calculate say $(3+4\epsilon)(5-6\epsilon)$:

```{r simplecheck}
(3+4*eps)*(5-6*eps)
```

Above, note the absence of a $-24\epsilon^2$ term, as $\epsilon^2=0$
for dual numbers.  

## Automatic differentiation

The basic idea is, given a function $f(\cdot)$ we define a dual
function [note to self: there must be a standard terminology here]
$\tilde{f}(\cdot)$:

$$\tilde{f}(x+\epsilon y) = f(x) + \epsilon yf'(x)$$

where $f'$ denotes the ordinary derivative $df/dx$.  The reason this
works is that applying the rule to composition of two functions, viz

$$
\tilde{g}\left(\tilde{f}(x+\epsilon)\right)=
g\left(f(x)+\epsilon f'(x)\right)=g(f(x))+\epsilon f'(x)g'(f(x))
$$

recovers the chain rule if we understand that
$f'(x)=\operatorname{Im}\left(\tilde{f}(x+\epsilon)\right)$.  We will
use this line of reasoning in the `clifford` package using a sequence
of examples of increasing sophistication.

## Simple powers

We observe that for functions including only the operations `+`, `-`
and `*`, we do not need to explicitly supply the derivative, as this
is automatically calculated by dual number arithmetic.  For example,
suppose we wish to differentiate $f(x)=x^n$.  Then we use the fact
that $f(x+\epsilon)=(x+\epsilon)^n=x^n+\epsilon x^{n-1}=f(x)+\epsilon
f'(x)$ [powers of $\epsilon$ above the first are discarded], and this
is transparently evaluated in the package.  Taking $n=5$ and $x=2$ as
an example:

```{r fivepower}
(2+eps)^5
```

and we see that this matches $f(2)=32$ [the real part] and
$f'(2)=5\cdot 2^4=80$ [the imaginary part].


## Polynomials

The same argument applies
to linear combinations of powers; now we define a simple polynomial,
and its derivative:

$$f(x) = 5x + 3x^5$$

In R idiom: 

```{r definasimplepoly}
f <- function(x){5*x + 3*x^5}
```

```{r, usef}
f(2.6 + eps)
```

The idiom does not use the symbolic result [viz $f'(x)=5+15x^4$]
anywhere: derivatives arise directly from Clifford formalism and
package consistency.  Above we see the return value of `f()` having
two components, the real part being the function value of about 369,
and the imaginary being the gradient of about 690.  We may be a little
more slick and create functions to extract the value and gradient
separately:

```{r defvalgrad}
valgrad <- function(f,x){
   jj <- f(x+eps)
   return(c(value=Re(jj),gradient=coeffs(Im(jj))))
}
```

Then

```{r usevalgrad}
valgrad(f,2.6)
```

Above, observe how the R parser [working with the `clifford` package],
when given `f()`, silently, seamlessly and correctly evaluates
$\tilde{f}$.  We may verify the result explicitly using symbolic form
for $f'$:

```{r, evaluatefatpi}
fdash <- function(x){5 + 15*x^4}
c(value=f(2.6), gradient=fdash(2.6))
```


### The chain rule

The next step would be to check the chain rule:
$(fg)'(x)=f'(g(x))g'(x)$.  As an example we will compose $f(x)$ as
defined above with $g(x) = x^2$:

```{r checkchain}
g <- function(x){x^2}
```

Then we can calculate $(fg)'=\frac{d}{dx}f(g(x))$ and
$(gf)'=\frac{d}{dx}g(f(x))$, evaluated at $x=1.6$:

```{r checkchaincliff}
x <- 1.6
f(g(x + eps))
g(f(x + eps))
```

## Transcendental and other functions

The techniques above are limited by the fact that only polynomial
functionality is implemented in the Clifford package; if we want to
work with transcendental functions we need to supply the derivative
explictly:

```{r, label=mydefs}
sin.clifford <- function(x){sin(Re(x)) + cos(Re(x))*Im(x)}
cos.clifford <- function(x){cos(Re(x)) - sin(Re(x))*Im(x)}
exp.clifford <- function(x){exp(Re(x)) + exp(Re(x))*Im(x)}
log.clifford <- function(x){log(Re(x)) + 1/(Re(x))*Im(x)}
```

Above we are using `S3` formalism to define methods for functions.
Thus if `x` is a Clifford object, `sin(x)` returns
$\widetilde{\sin}(x)=\sin x +\epsilon\sin x\cos x$.  As an example, we
will work with a slightly more complicated example,
$f_2(x)=\sin\left(\cos(x)+x^2\right)$:

```{r f2f2dash}
f2 <- function(x){sin(cos(x)+x^2)}
f2dash <- function(x){cos(cos(x)+ x^2)*(-sin(x)+2*x)}
```

Above we see $f_2(x)$ defined in R idiom; let us evaluate this
function and its gradient at $x=1.1$:

```{r f2valg}
c(value=f2(1.1), grad=f2dash(1.1))
valgrad(f2,1.1)
```

Above we again see close agreement.  Observe that dual number
formalism tracks the differential via the $e_1$ term automatically; it
effectively uses the chain rule to differentiate.


### Division

Division is somewhat more problematic.  Multiplication and addition
work directly but division may be included by defining a new operator
which we will call `%/%`:

```{r definemydivision}
`%/%` <- function(x,y){if(is.numeric(x) & is.numeric(y)){return(x/y)}
  xr <- Re(x)
  xd <- coeffs(Im(x))
  yr <- Re(y)
  yd <- coeffs(Im(y))
  return(xr/yr + eps*(xd*yr - xr*yd)/yr^2)
}
```

Then we can define loads of functions with reasonably nice idiom.
Here we consider $f_3(x)=\frac{\sin\left(1+x^2\right)}{2-e^x}$:

```{r usemydivision}
f3 <- function(x){sin(1+x^2) %/% (2-exp(x))}
valgrad(f3,pi)
```

Compare with the function and a numerical approximation to the
gradient at $x=\pi$:

```{r numericalderiv}
d <- 1e-6  # notional "small" value
c(value=f3(pi), grad=(f3(pi+d/2)-f3(pi-d/2))/d)
```

It is possible to implement division in a different way.  If we define

```{r defineinverse}
inv <- function(x){1/Re(x) -Im(x)/Re(x)^2}
```


Then `inv()` is a dual-ready version of $f(x)=x^{-1}$.  For example,
suppose we wish to find $f'(3)=-1/9$:

```{r useinv}
inv(3 + eps)
```

Above we see that $f(3)=\frac{1}{3}$ and $f'(3)=-\frac{1}{9}$.  We can
exploit the fact that `inv()` works in combination with other
functions as the chain rule applies to `inv()` as for any other
dual-ready function:

```{r useinvanother}
g <-  function(x){sin(1+x^2) * inv(2-exp(x))} # compare f(x) = sin(1+x^2) %/% (2-exp(x))
g(pi+eps)
```

## Multivariate case

The ideas above generalise seamlessley to the multivariate case.
Consider $f_4(x,y) = \frac{1+\sin(x+\cos y)}{2-e^{x+y}}$, at
$x=1.1,y=1.443$:

```{r, multivariate}
f4 <- function(x,y){(1+sin(x+cos(y))) %/% (2-exp(x+y))}
f4dx <- function(x,y,d=1e-6){(f4(x+d,y)-f4(x,y))/d}
f4dy <- function(x,y,d=1e-6){(f4(x,y+d)-f4(x,y))/d}
x <- 1.1
y <- 1.443
c(value=f4(x,y),"df/dx"=f4dx(x,y),"df/dy"=f4dy(x,y))
f4(x+eps,y)  # f(x,y) + eps*df/dx
f4(x,y+eps)  # f(x,y) + eps*df/dy
```

Again we see close agreement.  This suggests a nice way to evaluate,
say $\left(3\frac{\partial}{\partial x} + 5\frac{\partial}{\partial
y}\right)$:


```{r tryf4}
f4(x+3*eps,y+5*eps)
```

Compare the numerical estimate


```{r tryf4again}
(f4(x+3*d,y+5*d)-f4(x,y))/d
```

showing close agreement.

## A more realistic example


Here I use dual numbers to calculate a very small derivative, one that
is difficult to estimate any other way.  Suppose I define the
following sequence:

$$x_0=1$$

$$x_{n+1}=10-\alpha x^{1/5}-\beta x^{1/7},\qquad n\geqslant 0$$


```{r defineiterate}
iterate <- function(start,alpha,beta,n){
  x <- start
  for(i in seq_len(n)){
    x <- 10- alpha*exp(log(x)/5) - beta*exp(log(x)/7)  # exp() and log() defined above
  }
return(x)
}
```

I am interested in $x_{40}$ for $\alpha=\beta=1$:

```{r useiterate}
iterate(1,1,1,40)
```

OK but suppose I want the gradient, that is $\frac{\partial
x_{40}}{\partial\alpha}$.  One way to do it would be to use numerical
methods:

```{r useiterateagain}
(iterate(1,1+d,1,40) - iterate(1,1,1,40))/d  # d=1e-6
```

But is is soooooooooo much better to use dual numbers:

```{r label=iteratedual,cache=TRUE}
iterate(1,1+eps,1,40)
```

We get the value and the gradient (that is, $\frac{\partial
x_{40}}{\partial\alpha}$) and see that the two methods agree to
numerical precision.  But further, the technique is easy to apply to
derivatives with respect to the start value (`start` in R idiom or $S$
in algebra), here equal to 1.  Numerically:  

```{r itnumeric}
deriv <- function(delta){
   (iterate(1+delta,1,1,40)-iterate(1,1,1,40))/delta
}
```

We can try different values of `delta`:

```{r evaluatederiv}
deriv(0.1)
deriv(1000) # ludicrously large value for delta
```

Above, we see that in this case, the system "forgets" $S$ very quickly
and so any numerical technique essentially gives zero.  But dual
numbers use IEEE numbers for the gradient, which easily tracks the
derivative:

```{r label=iteratedualeps,cache=TRUE}
iterate(1+eps,1,1,40)
```

So AD gives a derivative of about $7\times 10^{-47}$, a value
difficult to obtain any other way.


## Lorenz butterfly

We have

$$\dot{x}=\sigma(y-x)\qquad\dot{y}=x(\rho-z)-y\qquad\dot{z}=xy-\beta z$$

with $\rho=28$, $\sigma=10$, $\beta=8/3$.  Setting up a simple Euler
scheme:

```{r setuplorenz}
rho <- 28
sigma <- 10
beta <- 8/3


dt <- 0.01
lorenz <- function(x,y,z,n,giveX=FALSE){
  if(giveX){xx <- rep(0,n)}
  for(i in seq_len(n)){
    x <- x + dt * sigma*(y-x)
    y <- y + dt * (x*(rho-z)-y)
    z <- z + dt * (x*y-beta*z)
    if(giveX){xx[i] <- x}
  }
  if(giveX){ return(xx)} else {return(x)}
}	
```

Above we have a simple forward Euler method for solving the Lorenz
butterfly equations.  We may plot the numerical solutions.

```{r lor100}
jj1 <- unlist(lorenz(1,1,1,3000,TRUE))
jj2 <- unlist(lorenz(1,1,1.001,3000,TRUE))
plot(jj1,type='l',col='red')
points(jj2,type='l',col='black')
```

Now, we ask "what is the derivative of the solution at the final
point, as a function of the start values for the parameter?  This is
absurdly easy with dual numbers; we run the simulation in exactly the
same way, but use $1+\epsilon$ as a start value:

```{r uselor2000,cache=TRUE}
lorenz(1,1,1+eps,2000)
```

We see that after 2000 timesteps the derivative is quite high, about
$1.4\times 10^{5}$, and after 5000 timesteps we have:

```{r uselor5000,cache=TRUE}
system.time(l5000 <- lorenz(1,1,1+eps,5000))
```

```{r}
l5000
```

Thus the gradient is about $-3\times 10^{17}$: chaos writ large.

## References

* Sartore L (2022). _`dual`: Automatic Differentiation with Dual  Numbers_. R package version 0.0.4,  <https://CRAN.R-project.org/package=dual>.
