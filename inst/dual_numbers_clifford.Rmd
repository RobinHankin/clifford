---
title: "Dual numbers implemented using Clifford algebra, and a case-study of automatic differentiation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("clifford")
```

<p style="text-align: right;">
![](`r system.file("help/figures/clifford.png", package = "clifford")`){width=10%}
</p>


From wikipedia, dual numbers are expressions of the form
$a+b\epsilon$, with $a,b\in\mathcal{R}$ and $\epsilon^2=0$.  If we
define

$$\begin{eqnarray}
(a+b\epsilon) + (c+d\epsilon) &=& (a+c) + (b+d)\epsilon\\
(a+b\epsilon)(c+d\epsilon) &=& ac + (ad+bc)\epsilon
\end{eqnarray}$$


then we recover distributivity and associativity.  The Clifford
representation is easy, first setting the signature to zero:

```{r defineeps}
library(clifford)
signature(0)
options(maxdim = 1) # safety measure
eps <- clifford(list(1))  # 'eps'  is shorthand for epsilon
eps   # regular Clifford object
drop(eps^2)  # check that eps^2==0
```

Then, to calculate say $(3+4\epsilon)(5-6\epsilon)$:

```{r simplecheck}
(3+4*eps)*(5-6*eps)
```

Numerical verification of algebraic identities is straightforward:

```{r numverif}
rdual <- function(...){rnorm(1) + rnorm(1)*eps}
x <- rdual()
y <- rdual()
z <- rdual()
```

Then

```{r checkdistassoc}
Mod(x*(y*z) - (x*y)*z)
Mod(x*(y+z) - (x*y+x*z))
Mod((x+y)*z - (x*z+y*z))
```

The above shows zero to within numerical tolerance.


## Automatic differentiation

*NOTE* This vignette shows a use-case for dual numbers, as applied to
 automatic differentiation.  If you want to actually use AD, I would
 strongly recommend you consider the excellent `dual` package which
 offers a nice user interface, efficient coding, and lots of examples.


First we define a simple polynomial, and its derivative:

```{r definasimplepoly}
f <- function(x){5*x + 3*x^5}
fdash <- function(x){5 + 15*x^4}
```

If we evaluate these at $x=2.6$, say:

```{r, evaluatefatpi}
c(value=f(2.6), gradient=fdash(2.6))
```

Above we see that the function evaluates to about 369 and the
gradient is about 690.  However, we observe that `f()` may be
evaluated at elements of a Clifford algebra:

```{r, usef}
f(2.6 + eps)
```

Above we see the result having two components, the real part being the
function value and the imaginary being the gradient; there is close
agreement between the function value and gradient using the algebraic
method and the dual number method.  We may be a little more slick and
create functions to extract the value and gradient separately:

```{r defvalgrad}
valgrad <- function(f,x){
   jj <- f(x+eps)
   return(c(value=Re(jj),gradient=coeffs(Im(jj))))
}
```

Then

```{r usevalgrad}
valgrad(f,2.6)
```

However, This technique is limited by the fact that only polynomial
functionality is implemented in the Clifford package; if we want to
work with transcendental functions we need to supply the derivative
explictly:


```{r, label=mydefs}
sin.clifford <- function(x){sin(Re(x)) + cos(Re(x))*Im(x)}
cos.clifford <- function(x){cos(Re(x)) - sin(Re(x))*Im(x)}
exp.clifford <- function(x){exp(Re(x)) + exp(Re(x))*Im(x)}
log.clifford <- function(x){log(Re(x)) + 1/(Re(x))*Im(x)}
```

We will work with $f_2(x)=\sin\left(\cos(x)+x^2\right)$:

```{r f2f2dash}
f2 <- function(x){sin(cos(x)+x^2)}
f2dash <- function(x){cos(cos(x)+ x^2)*(-sin(x)+2*x)}
```

Above we see $f_2(x)$ defined in R idiom; let us evaluate this
function and its gradient at $x=1.1$:

```{r f2valg}
c(value=f2(1.1), grad=f2dash(1.1))
valgrad(f2,1.1)
```

Above we again see close agreement.  Observe that dual number
formalism tracks the differential via the $e_1$ term automatically; it
effectively uses the chain rule to differentiate.
p
Division is somewhat more problematic.  Multiplication and addition
work directly but division requires a new operator which we will call
`%/%`:

```{r definemydivision}
`%/%` <- function(x,y){if(is.numeric(x) & is.numeric(y)){return(x/y)}
  xr <- Re(x)
  xd <- coeffs(Im(x))
  yr <- Re(y)
  yd <- coeffs(Im(y))
  return(xr/yr + eps*(xd*yr - xr*yd)/yr^2)
}
```

Then we can define loads of functions with reasonably nice idiom.
Here we consider $f_3(x)=\frac{\sin\left(1+x^2\right)}{2-e^x}$:

```{r usemydivision}
f3 <- function(x){sin(1+x^2) %/% (2-exp(x))}
valgrad(f3,pi)
```

Compare with the function and a numerical approximation to the
gradient at $x=\pi$:

```{r numericalderiv}
d <- 1e-6  # notional "small" value
c(value=f3(pi), grad=(f3(pi+d/2)-f3(pi-d/2))/d)
```

## Multivariate case

The ideas above generalise seamlessley to the multivariate case.
Consider $f_4(x,y) = \frac{1+\sin(x+\cos y)}{2-e^{x+y}}$, at
$x=1.1,y=1.443$:

```{r, multivariate}
f4 <- function(x,y){(1+sin(x+cos(y))) %/% (2-exp(x+y))}
f4dx <- function(x,y,d=1e-6){(f4(x+d,y)-f4(x,y))/d}
f4dy <- function(x,y,d=1e-6){(f4(x,y+d)-f4(x,y))/d}
x <- 1.1
y <- 1.443
c(value=f4(x,y),"df/dx"=f4dx(x,y),"df/dy"=f4dy(x,y))
f4(x+eps,y)  # f(x,y) + eps*df/dx
f4(x,y+eps)  # f(x,y) + eps*df/dy
```

Again we see close agreement.  This suggests a nice way to evaluate,
say $\left(3\frac{\partial}{\partial x} + 5\frac{\partial}{\partial
y}\right)$:


```{r tryf4}
f4(x+3*eps,y+5*eps)
```

Compare the numerical estimate


```{r tryf4again}
(f4(x+3*d,y+5*d)-f4(x,y))/d
```

showing close agreement.

## A more realistic example

Suppose I define the following sequence:

$$x_0=1$$

$$x_{n+1}=10-\alpha x^{1/5}-\beta x^{1/7},\qquad n\geqslant 0$$


```{r defineiterate}
iterate <- function(start,alpha,beta,n){
  x <- start
  for(i in seq_len(n)){
    x <- 10- alpha*exp(log(x)/5) - beta*exp(log(x)/7)  # exp() and log() defined above
  }
return(x)
}
```


I am interested in $x_{20}$ for $\alpha=\beta=1$:

```{r useiterate}
iterate(1,1,1,20)
```

OK but suppose I want the gradient, that is $\frac{\partial
x_{10}}{\partial\alpha}$.

One way to do it would be to use numerical methods:

```{r useiterateagain}
(iterate(1,1+d,1,20) - iterate(1,1,1,20))/d  # d=1e-6
```


But is is soooooooooo much better to use dual numbers:

```{r label=iteratedual,cache=TRUE}
iterate(1,1+eps,1,20)
```

We get the value and the gradient.  The technique is easy to apply to
derivatives with respect to the start value.  In this case, the system
"forgets" the start value very quickly and so any numerical technique
essentially gives zero:

```{r itnumeric}
(iterate(1+d,1,1,10)-iterate(1,1,1,10))/d
```

But dual numbers use IEEE numbers for the gradient, which easily
tracks the derivative:

```{r label=iteratedualeps,cache=TRUE}
iterate(1+eps,1,1,20)
```

## Lorenz butterfly

We have

$$\dot{x}=\sigma(y-x)\qquad\dot{y}=x(\rho-z)-y\qquad\dot{z}=xy-\beta z$$

with $\rho=28$, $\sigma=10$, $\beta=8/3$.  Setting up a simple Euler
scheme:

```{r setuplorenz}
rho <- 28
sigma <- 10
beta <- 8/3
xdot <- function(x,y,z){sigma*(y-x)}
ydot <- function(x,y,z){x*(rho-z)-y}
zdot <- function(x,y,z){x*y-beta*z}

dt <- 0.01
lorenz <- function(x,y,z,n,giveX=FALSE){
  if(giveX){xx <- rep(0,n)}
  for(i in seq_len(n)){
    xnew <- x + dt*xdot(x,y,z)
    ynew <- y + dt*ydot(x,y,z)
    znew <- z + dt*zdot(x,y,z)
    x <- xnew
    y <- ynew
    z <- znew
    if(giveX){xx[i] <- x}
  }
  if(giveX){  return(xx)} else {return(x)}
}	
```

Then

```{r lor100}
jj1 <- unlist(lorenz(1,1,1,1000,TRUE))
jj2 <- unlist(lorenz(1,1,1.01,1000,TRUE))
plot(jj1)
points(jj2)
```

```{r uselor1000, cache=TRUE}
system.time(l300 <- lorenz(1,1,1+eps,300))
system.time(l400 <- lorenz(1,1,1+eps,400))
```

```{r show1000}
l300
l400
```





```{r uselor700,cache=TRUE}
system.time(l700 <- lorenz(1,1,1+eps,700))
```

```{r}
l700
```

```{r}
d <- 1e-5
(lorenz(1,1,1+d,700)-lorenz(1,1,1,700))/d
```