---
title: "Dual numbers implemented using Clifford algebra, and a case-study of automatic differentiation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("clifford")
```

<p style="text-align: right;">
![](`r system.file("help/figures/clifford.png", package = "clifford")`){width=10%}
</p>


From wikipedia, dual numbers are expressions of the form
$a+b\epsilon$, with $a,b\in\mathcal{R}$ and $\epsilon^2=0$.  If we
define

$$\begin{eqnarray}
(a+b\epsilon) + (c+d\epsilon) &=& (a+c) + (b+d)\epsilon\\
(a+b\epsilon)(c+d\epsilon) &=& ac + (ad+bc)\epsilon
\end{eqnarray}$$


then we recover distributivity and associativity.  The Clifford
representation is easy, first setting the signature to zero:

```{r defineeps}
library(clifford)
signature(0)
options(maxdim = 1) # safety measure
eps <- clifford(list(1))  # 'eps'  is shorthand for epsilon
eps   # regular Clifford object
drop(eps^2)  # check that eps^2==0
```

Then, to calculate say $(3+4\epsilon)(5-6\epsilon)$:

```{r simplecheck}
(3+4*eps)*(5-6*eps)
```

Numerical verification of algebraic identities is straightforward:

```{r numverif}
rdual <- function(...){rnorm(1) + rnorm(1)*eps}
x <- rdual()
y <- rdual()
z <- rdual()
```

Then

```{r checkdistassoc}
Mod(x*(y*z) - (x*y)*z)
Mod(x*(y+z) - (x*y+x*z))
Mod((x+y)*z - (x*z+y*z))
```

The above shows zero to within numerical tolerance.


## Automatic differentiation

*NOTE* This vignette shows a use-case for dual numbers, as applied to
 automatic differentiation.  If you want to actually use AD, I would
 strongly recommend you consider the excellent `dual` package which
 offers a nice user interface, efficient coding, and lots of examples.

The basic idea is, given a function $f(\cdot)$ we define a dual
function [note to self: there must be a standard terminology here]
$\tilde{f}(\cdot)$:

$$\tilde{f}(x+\epsilon y) = f(x) + \epsilon yf'(x)$$

where $f'$ denotes the ordinary derivative $df/dx$.  The reason this
works is that applying the rule to composition of two functions, viz

$$
\tilde{g}\left(\tilde{f}(x+\epsilon)\right)=
g\left(f(x)+\epsilon f'(x)\right)=g(f(x))+\epsilon f'(x)g'(f(x))
$$

recovers the chain rule if we understand that
$f'(x)=\operatorname{Im}\left(\tilde{f}(x+\epsilon)\right)$.  We will
use this line of reasoning in the `clifford` package using a sequence
of examples of increasing sophistication.  First we define a simple
polynomial, and its derivative:

$$f(x) = 5x + 3x^5\qquad f'(x) = 5 + 15x^4$$

In R idiom:

```{r definasimplepoly}
f <- function(x){5*x + 3*x^5}
fdash <- function(x){5 + 15*x^4}
```

If we evaluate these at $x=2.6$, say:

```{r, evaluatefatpi}
c(value=f(2.6), gradient=fdash(2.6))
```

Above we see that the function evaluates to about 369 and the
gradient is about 690.  However, we observe that `f()` may be
evaluated at elements of a Clifford algebra:

```{r, usef}
f(2.6 + eps)
```

Above, observe how the R parser [working with the `clifford` package],
when given `f()`, silently, seamlessly and correctly evaluates
$\tilde{f}$.  The idiom does not use `fdash()` anywhere: derivatives
arise directly from Clifford formalism and package consistency.  Above
we see the return value of `f()` having two components, the real part
being the function value and the imaginary being the gradient; there
is close agreement between the function value and gradient using the
algebraic method and the dual number method.  We may be a little more
slick and create functions to extract the value and gradient
separately:

```{r defvalgrad}
valgrad <- function(f,x){
   jj <- f(x+eps)
   return(c(value=Re(jj),gradient=coeffs(Im(jj))))
}
```

Then

```{r usevalgrad}
valgrad(f,2.6)
```

The next step would be to check the chain rule.  We will compose $f$
as defined above with $g(x) = x^2$:

```{r checkchain}
g <- function(x){x^2}
```

Then we can calculate $(fg)'=\frac{d}{dx}f(g(x))$ and
$(gf)'=\frac{d}{dx}g(f(x))$, evaluated at $x=1.6$:

```{r checkchaincliff}
x <- 1.6
f(g(x + eps))
g(f(x + eps))
```

However, This technique is limited by the fact that only polynomial
functionality is implemented in the Clifford package; if we want to
work with transcendental functions we need to supply the derivative
explictly:

```{r, label=mydefs}
sin.clifford <- function(x){sin(Re(x)) + cos(Re(x))*Im(x)}
cos.clifford <- function(x){cos(Re(x)) - sin(Re(x))*Im(x)}
exp.clifford <- function(x){exp(Re(x)) + exp(Re(x))*Im(x)}
log.clifford <- function(x){log(Re(x)) + 1/(Re(x))*Im(x)}
```

Above we are using `S3` formalism to define methods for functions.
Thus if `x` is a Clifford object, `sin(x)` returns
$\widetilde{\sin}(x)=\sin x +\epsilon\sin x\cos x$.  As an example, we
will work with a slightly more complicated example,
$f_2(x)=\sin\left(\cos(x)+x^2\right)$:

```{r f2f2dash}
f2 <- function(x){sin(cos(x)+x^2)}
f2dash <- function(x){cos(cos(x)+ x^2)*(-sin(x)+2*x)}
```

Above we see $f_2(x)$ defined in R idiom; let us evaluate this
function and its gradient at $x=1.1$:

```{r f2valg}
c(value=f2(1.1), grad=f2dash(1.1))
valgrad(f2,1.1)
```

Above we again see close agreement.  Observe that dual number
formalism tracks the differential via the $e_1$ term automatically; it
effectively uses the chain rule to differentiate.
p
Division is somewhat more problematic.  Multiplication and addition
work directly but division requires a new operator which we will call
`%/%`:

```{r definemydivision}
`%/%` <- function(x,y){if(is.numeric(x) & is.numeric(y)){return(x/y)}
  xr <- Re(x)
  xd <- coeffs(Im(x))
  yr <- Re(y)
  yd <- coeffs(Im(y))
  return(xr/yr + eps*(xd*yr - xr*yd)/yr^2)
}
```

Then we can define loads of functions with reasonably nice idiom.
Here we consider $f_3(x)=\frac{\sin\left(1+x^2\right)}{2-e^x}$:

```{r usemydivision}
f3 <- function(x){sin(1+x^2) %/% (2-exp(x))}
valgrad(f3,pi)
```

Compare with the function and a numerical approximation to the
gradient at $x=\pi$:

```{r numericalderiv}
d <- 1e-6  # notional "small" value
c(value=f3(pi), grad=(f3(pi+d/2)-f3(pi-d/2))/d)
```

## Multivariate case

The ideas above generalise seamlessley to the multivariate case.
Consider $f_4(x,y) = \frac{1+\sin(x+\cos y)}{2-e^{x+y}}$, at
$x=1.1,y=1.443$:

```{r, multivariate}
f4 <- function(x,y){(1+sin(x+cos(y))) %/% (2-exp(x+y))}
f4dx <- function(x,y,d=1e-6){(f4(x+d,y)-f4(x,y))/d}
f4dy <- function(x,y,d=1e-6){(f4(x,y+d)-f4(x,y))/d}
x <- 1.1
y <- 1.443
c(value=f4(x,y),"df/dx"=f4dx(x,y),"df/dy"=f4dy(x,y))
f4(x+eps,y)  # f(x,y) + eps*df/dx
f4(x,y+eps)  # f(x,y) + eps*df/dy
```

Again we see close agreement.  This suggests a nice way to evaluate,
say $\left(3\frac{\partial}{\partial x} + 5\frac{\partial}{\partial
y}\right)$:


```{r tryf4}
f4(x+3*eps,y+5*eps)
```

Compare the numerical estimate


```{r tryf4again}
(f4(x+3*d,y+5*d)-f4(x,y))/d
```

showing close agreement.

## A more realistic example


Here I use dual numbers to calculate a very small derivative, one that
is difficult to estimate any other way.  Suppose I define the
following sequence:

$$x_0=1$$

$$x_{n+1}=10-\alpha x^{1/5}-\beta x^{1/7},\qquad n\geqslant 0$$


```{r defineiterate}
iterate <- function(start,alpha,beta,n){
  x <- start
  for(i in seq_len(n)){
    x <- 10- alpha*exp(log(x)/5) - beta*exp(log(x)/7)  # exp() and log() defined above
  }
return(x)
}
```

I am interested in $x_{20}$ for $\alpha=\beta=1$:

```{r useiterate}
iterate(1,1,1,20)
```

OK but suppose I want the gradient, that is $\frac{\partial
x_{10}}{\partial\alpha}$.  One way to do it would be to use numerical
methods:

```{r useiterateagain}
(iterate(1,1+d,1,20) - iterate(1,1,1,20))/d  # d=1e-6
```


But is is soooooooooo much better to use dual numbers:

```{r label=iteratedual,cache=TRUE}
iterate(1,1+eps,1,20)
```

We get the value and the gradient.  The technique is easy to apply to
derivatives with respect to the start value.  In this case, the system
"forgets" the start value very quickly and so any numerical technique
essentially gives zero:

```{r itnumeric}
(iterate(1+d,1,1,10)-iterate(1,1,1,10))/d
```

But dual numbers use IEEE numbers for the gradient, which easily
tracks the derivative:

```{r label=iteratedualeps,cache=TRUE}
iterate(1+eps,1,1,20)
```

## Lorenz butterfly

We have

$$\dot{x}=\sigma(y-x)\qquad\dot{y}=x(\rho-z)-y\qquad\dot{z}=xy-\beta z$$

with $\rho=28$, $\sigma=10$, $\beta=8/3$.  Setting up a simple Euler
scheme:

```{r setuplorenz}
rho <- 28
sigma <- 10
beta <- 8/3


dt <- 0.01
lorenz <- function(x,y,z,n,giveX=FALSE){
  if(giveX){xx <- rep(0,n)}
  for(i in seq_len(n)){
    x <- x + dt * sigma*(y-x)
    y <- y + dt * (x*(rho-z)-y)
    z <- z + dt * (x*y-beta*z)
    if(giveX){xx[i] <- x}
  }
  if(giveX){ return(xx)} else {return(x)}
}	
```

Above we have a simple forward Euler method for solving the Lorenz
butterfly equations.  We may plot the numerical solutions.

```{r lor100}
jj1 <- unlist(lorenz(1,1,1,3000,TRUE))
jj2 <- unlist(lorenz(1,1,1.001,3000,TRUE))
plot(jj1,type='l',col='red')
points(jj2,type='l',col='black')
```

Now, we ask "what is the derivative of the solution at the final
point, as a function of the start values for the parameter?  This is
absurdly easy with dual numbers; we run the simulation in exactly the
same way, but use $1+\epsilon$ as a start value:

```{r uselor2000,cache=TRUE}
lorenz(1,1,1+eps,2000)
```

We see that after 2000 timesteps the derivative is quite high, about
$1.4\times 10^{5}$, and after 5000 timesteps we have:

```{r uselor5000,cache=TRUE}
system.time(l5000 <- lorenz(1,1,1+eps,5000))
```

```{r}
l5000
```

Thus the gradient is about $-3\times 10^{17}$: chaos writ large.