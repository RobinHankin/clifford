---
title: "Dual numbers implemented using Clifford algebra, and a case-study of automatic differentiation"
author: "Robin K. S. Hankin"
output: html_vignette
bibliography: clifford.bib
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{dual_numbers}
  %\usepackage[utf8]{inputenc}
---


```{r setup, include=FALSE}
options(rmarkdown.html_vignette.check_title = FALSE) 
knitr::opts_chunk$set(echo = TRUE)
library("clifford")
set.seed(0)
```

```{r, label=hexstickerplotter, out.width='15%', out.extra='style="float:right; padding:10px"',echo=FALSE}
knitr::include_graphics(system.file("help/figures/clifford.png", package = "clifford"))
```

(takes a couple of hours to run without cache)

To cite the clifford package in publications, use @hankin2022_clifford.

*NOTE* This vignette shows a use-case for dual numbers, as applied to
 automatic differentiation.  All it really does is illustrate that
 dual numbers are a special case of Clifford algebra.  If you want to
 actually use AD, I would strongly recommend you consider the
 excellent `dual` package [@sartore2022], which offers a nice user
 interface, efficient coding, and lots of examples.

## Dual numbers

Dual numbers ${\mathbb D}$ are expressions of the form $a+b\epsilon$,
with $a,b\in\mathbb{R}$ and $\epsilon^2=0$.  If we define

$$\begin{eqnarray}
(a+b\epsilon) + (c+d\epsilon) &=& (a+c) + (b+d)\epsilon\\
(a+b\epsilon)(c+d\epsilon) &=& ac + (ad+bc)\epsilon
\end{eqnarray}$$


then we recover distributivity, commutativity, and associativity.  The
Clifford representation is easy, first setting the signature to zero:

```{r defineeps}
library(clifford)
signature(0)
options(maxdim = 1) # safety measure
eps <- clifford(list(1))  # 'eps'  is shorthand for epsilon
eps   # regular Clifford object
drop(eps^2)  # check that eps^2==0
```

Then, to calculate say $(3+4\epsilon)(5-6\epsilon)$:

```{r simplecheck}
(3+4*eps)*(5-6*eps)
```

Above, note the absence of a $-24\epsilon^2$ term, as $\epsilon^2=0$
for dual numbers.  

## Matrix representation

To represent dual numbers with $2\times 2$ matrices over the reals we
define a function $\phi\colon\mathbb{D}\longrightarrow\mathbb{R}$ as
follows:

$$
\phi(a + b\epsilon)=\begin{bmatrix}a&b\\0&a\end{bmatrix}.
$$

This mapping is a homomorphism between dual numbers and their matrix
equivalents, for if $x=a+b\epsilon$ and $y=c+d\epsilon$, we have

$$
\phi(xy)=\phi((a+b\epsilon)\cdot(c+d\epsilon))=\phi(ac+(ad+bc)\epsilon)=
\begin{bmatrix}ac&ad+bc\\0&ac\end{bmatrix}
$$

while

$$
\phi(x)\phi(y)=
\begin{bmatrix}a&b\\0&a\end{bmatrix}\begin{bmatrix}c&d\\0&c\end{bmatrix}=
\begin{bmatrix}ac&ad+bc\\0&ac\end{bmatrix}=\phi(xy).
$$

Numerical verification of this is straightforward:


```{r label=verifyhomosetup}
as.matrix.clifford <- function(x){matrix(c(Re(x),0,coeffs(Im(x)),Re(x)),2,2)}
x <- rnorm(1) + eps*rnorm(1)
y <- rnorm(1) + eps*rnorm(1)
```


```{r label=verifyhomoactual}
as.matrix(x) %*% as.matrix(y) - as.matrix(x*y)
```

above we see that $\phi(xy)$ and $\phi(x)\phi(y)$ agree to within
numerical precision.  Further, elementary matrix algebra gives us that
$\begin{bmatrix}a&b\\0&a\end{bmatrix}^{-1}=\begin{bmatrix}a^{-1}&-ba^{-2}\\0&a^{-1}\end{bmatrix}$
so we may define $(a+b\epsilon)^{-1}=a^{-1}-ba^{-2}\epsilon$, provided
$a\neq 0$.

## Automatic differentiation: the chain rule

The basic idea is, given a function
$f\colon\mathbb{R}\longrightarrow\mathbb{R}$, we define a dual
function [note to self: there must be a standard terminology here]
$\tilde{f}\colon\mathbb{D}\longrightarrow\mathbb{D}$ as follows:

$$\tilde{f}(x+\epsilon y) = f(x) + \epsilon yf'(x)$$

where $f'$ denotes the ordinary derivative $df/dx$.  The reason this
works is that applying the rule to composition of two functions, viz

$$
\begin{align}
\tilde{g}\left(\tilde{f}(x+\epsilon)\right) &= g\left(f(x)+\epsilon f'(x)\right)\\
&= g(f(x))+\epsilon f'(x)g'(f(x))
\end{align}
$$

recovers the chain rule [viz $\frac{d}{dx}f(g(x))=f'(g(x))g'(x)$] if
we understand that
$f'(x)=\operatorname{Im}\left(\tilde{f}(x+\epsilon)\right)$.  We will
use this line of reasoning in the `clifford` package using a sequence
of examples of increasing sophistication.

## Simple powers

We observe that for functions including only the operations `+`, `-`
and `*`, we do not need to explicitly supply the derivative, as this
is automatically calculated by dual number arithmetic.  For example,
suppose we wish to differentiate $f(x)=x^n$.  Then we use the fact
that $f(x+\epsilon)=(x+\epsilon)^n=x^n+\epsilon nx^{n-1}=f(x)+\epsilon
f'(x)$ [powers of $\epsilon$ above the first are discarded], and this
is transparently evaluated in the package.  Taking $n=5$ and $x=2$ as
an example:

```{r fivepower}
(2+eps)^5
```

and we see that this matches $f(2)=32$ [the real part] and
$f'(2)=5\cdot 2^4=80$ [the imaginary part].


## Polynomials

The same argument applies
to linear combinations of powers; now we define a simple polynomial,
and its derivative:

$$f(x) = 5x + 3x^5$$

In R idiom: 

```{r definasimplepoly}
f <- function(x){5*x + 3*x^5}
```

```{r, usef}
f(2.6 + eps)
```

The idiom does not use the symbolic result [viz $f'(x)=5+15x^4$]
anywhere: derivatives arise directly from Clifford formalism and
package consistency.  Above we see the return value of `f()` having
two components, the real part being the function value of about 369,
and the imaginary being the gradient of about 690.  We may be a little
more slick and create functions to extract the value and gradient
separately, using Clifford functionality directly:

```{r defvalgrad}
valgrad <- function(f,x){
   jj <- f(x+eps)
   return(c(value=Re(jj),gradient=coeffs(Im(jj))))
}
```

Then

```{r usevalgrad}
valgrad(f,2.6)
```

Above, observe how the R parser [working with the `clifford` package],
when given `f()`, silently, seamlessly and correctly evaluates
$\tilde{f}$.  We may verify the result explicitly using symbolic form
for $f'$:

```{r, evaluatefatpi}
fdash <- function(x){5 + 15*x^4}
c(value=f(2.6), gradient=fdash(2.6))
```


### Numerical verification of the chain rule

The next step would be to check the chain rule:
$(fg)'(x)=f'(g(x))g'(x)$.  As an example we will compose
$f(x)=5x+3x^5$ as defined above with $g(x) = x^2$:

```{r checkchain}
g <- function(x){x^2}
```

Then we can calculate $(fg)'=\frac{d}{dx}f(g(x))$ and
$(gf)'=\frac{d}{dx}g(f(x))$, evaluated at $x=1.6$:

```{r checkchaincliff}
x <- 1.6
valgrad(function(x){f(g(x))},1.6)
c(value=f(g(x)),gradient=(f(g(x+1e-6))-f(g(x))) / 1e-6)
```

Above we see close agreemennt between the dual number result and the
direct numerical estimate using $\delta x=10^{-6}$.


## Transcendental and other functions

The techniques above are limited by the fact that only polynomial
functionality is implemented in the Clifford package; if we want to
work with transcendental functions we need to supply the derivative
explicitly:

```{r, label=mydefs}
sin.clifford <- function(x){sin(Re(x)) + cos(Re(x))*Im(x)}
cos.clifford <- function(x){cos(Re(x)) - sin(Re(x))*Im(x)}
exp.clifford <- function(x){exp(Re(x)) + exp(Re(x))*Im(x)}
log.clifford <- function(x){log(Re(x)) + 1/(Re(x))*Im(x)}
`^.clifford` <- function(x,y){exp(y*log(x))}
```

Above we are using `S3` formalism to define methods for functions.
Thus if `x` is a Clifford object, `sin(x)` returns
$\widetilde{\sin}(x)=\sin x +\epsilon\cos x$.  As an example, we will
work with a slightly more complicated example,
$f_2(x)=\sin\left(\cos(x)+x^2\right)$:

```{r f2f2dash}
f2 <- function(x){sin(cos(x)+x^2)}
f2dash <- function(x){cos(cos(x)+ x^2)*(-sin(x)+2*x)}
```

Above we see $f_2(x)$ defined in R idiom; let us evaluate this
function and its gradient at $x=1.1$:

```{r f2valsymb}
c(value=f2(1.1), grad=f2dash(1.1))  # symbolic value
```

Above we see the value of the function and its derivative as computed
directly.  But we can compare this with the result of using Clifford
formalism:

```{r f2valcliff}
valgrad(f2,1.1)                  
```

Above we again see close agreement.  Observe that dual number
formalism tracks the differential via the $e_1$ term automatically; it
effectively uses the chain rule to differentiate.  The same technique
can be used for functions of any complexity.

# Division

Division is somewhat more problematic.  Multiplication and addition
work without modification, but division requires the definition of a
new `S3` method:

```{r definemydivision}
`/.clifford` <- function(x,y){
    if(!is.clifford(y)){return(x*y^(-1))}
    xr <- Re(x)
    jj <- coeffs(Im(x))
    xd <- ifelse(length(jj)==0,0,jj)
    yr <- Re(y)
    yd <- coeffs(Im(y))
    return(xr/yr + ((xd*yr - xr*yd)/yr^2)*eps)
}
```

Above we see a representation of the quotient rule in calculus: if
$h(x)=\frac{f(x)}{g(x)}$, then $h'(x)=
\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}$, and this appears in the $e_1$
term of the return value.  With this, we can define many functions
with reasonably nice idiom.  Here we consider
$f_3(x)=\frac{\sin\left(1+x^2\right)}{2-e^x}$:

```{r usemypercentdivision}
f3 <- function(x){sin(1+x^2) / (2-exp(x))}
valgrad(f3,pi)
```

Compare with the function and a numerical approximation to the
gradient at $x=\pi$:

```{r numericalderiv}
d <- 1e-6  # notional "small" value
c(value=f3(pi), grad=(f3(pi+d/2)-f3(pi-d/2))/d)
```

## Alternative implementation of division using inverses

It is possible to implement division in a different way.  If we define

```{r defineinverse}
inv <- function(x){1/Re(x) -eps*(coeffs(Im(x))/Re(x)^2)}
```

Then `inv()` is a dual-ready version of $f(x)=x^{-1}$ with
$f'(x)=-1/x^2$.  For example, suppose we wish to find $f'(3)=-1/9$:

```{r useinv}
inv(3 + eps)
```

Above we see that $\operatorname{inv}(3)=\frac{1}{3}$ and
$\operatorname{inv}'(3)=-\frac{1}{9}$ (formally one would write
$\left.\operatorname{inv}'(x)\right|_{x=3}=-\frac{1}{9}$).  We can
exploit the fact that `inv()` works in combination with other
functions as the chain rule applies to `inv()` as for any other
dual-ready function.  Here I use R idiom to define `g()` which returns
$f(x)=\frac{\sin(1+x^2)}{2-\exp(x)}$ as used above, but using `inv()`
instead of `/.clifford()`:

```{r useinvanother}
g <-  function(x){sin(1+x^2) * inv(2-exp(x))} # compare f(x) = sin(1+x^2) / (2-exp(x))
g(pi+eps)
```

We can further verify that this new division operator works by considering $\sec(x)=1/\cos(x)$ in different ways:

```{r checksec}
sec1 <- function(x){1/cos(Re(x)) + (sin(Re(x))/cos(Re(x))^2)*Im(x)}
sec2 <- function(x){as.clifford(1) / cos(x)}  # coercion needed to dispatch correctly
sec3 <- function(x){inv(cos(x))}
list(
    sec1(0.3+eps),
    sec2(0.3+eps),
    sec3(0.3+eps)
    )
```

Above we see agreement between `sec1()`, defined using the algebraic
derivative, `sec2()`, defined using the `/.clifford()` and `sec3()`,
defined using the `inv()` function.


## Multivariate case

The ideas above generalise seamlessly to the multivariate case.
Consider $f_4(x,y) = \frac{1+\sin(x+\cos y)}{2-e^{x+y}}$, at
$x=1.1,y=1.443$:

```{r, multivariate}
f4 <- function(x,y){(1+sin(x+cos(y))) / (2-exp(x+y))}
f4dx <- function(x,y,d=1e-6){(f4(x+d,y)-f4(x,y))/d}  # numerical estimate
f4dy <- function(x,y,d=1e-6){(f4(x,y+d)-f4(x,y))/d}  # numerical estimate
x <- 1.1
y <- 1.443
c(value=f4(x,y),"df/dx"=f4dx(x,y),"df/dy"=f4dy(x,y))
f4(x+eps,y)  # f(x,y) + eps*df/dx
f4(x,y+eps)  # f(x,y) + eps*df/dy
```

Again we see close agreement.  This suggests a nice way to evaluate,
say, $\left(3\frac{\partial}{\partial x} + 5\frac{\partial}{\partial
y}\right)$:


```{r tryf4}
f4(x+3*eps,y+5*eps)
```

Compare the numerical estimate


```{r tryf4again}
(f4(x+3*d,y+5*d)-f4(x,y))/d
```

showing close agreement (observe that `f4()`, although intended to be
used with dual numbers, behaves sensibly when given a numeric
argument).

## A more realistic example


Here I use dual numbers to calculate a very small derivative, one that
is difficult to estimate any other way.  Suppose I define the
following sequence:

$$x_0=1$$

$$x_{n+1}=10-\alpha x^{1/5}-\beta x^{1/7},\qquad n\geqslant 0$$


```{r defineiterate}
iterate <- function(start,alpha,beta,n){
  x <- start
  for(i in seq_len(n)){
    x <- 10- alpha*exp(log(x)/5) - beta*exp(log(x)/7)  # exp() and log() defined above
  }
return(x)
}
```

I am interested in $x_{40}$ for $\alpha=\beta=1$:

```{r useiterate}
iterate(1,1,1,40)
```

OK but suppose I want the gradient, that is $\frac{\partial
x_{40}}{\partial\alpha}$.  One way to do it would be to use numerical
methods:

```{r useiterateagain}
(iterate(1,1+d,1,40) - iterate(1,1,1,40))/d  # d=1e-6
```

But is is soooooooooo much better to use dual numbers:

```{r label=iteratedual,cache=TRUE}
iterate(1,1+eps,1,40)
```

We get the value and the gradient (that is, $\frac{\partial
x_{40}}{\partial\alpha}$) and see that the two methods agree to
numerical precision.  But further, the technique is easy to apply to
derivatives with respect to the start value (`start` in R idiom or $S$
in algebra), here equal to 1.  Numerically:  

```{r itnumeric}
deriv <- function(delta){
   (iterate(1+delta,1,1,40)-iterate(1,1,1,40))/delta
}
```

We can try different values of `delta`:

```{r evaluatederiv}
deriv(0.1)
deriv(1000) # ludicrously large value for delta
```

Above, we see that in this case, the system "forgets" $S$ very quickly
and so any numerical technique essentially gives zero.  But dual
numbers use IEEE numbers for the gradient, which easily tracks the
derivative:

```{r label=iteratedualeps,cache=TRUE}
iterate(1+eps,1,1,40)
```

So AD gives a derivative of about $7\times 10^{-47}$, a value
difficult to obtain any other way.


## Lorenz butterfly

We have

$$\dot{x}=\sigma(y-x)\qquad\dot{y}=x(\rho-z)-y\qquad\dot{z}=xy-\beta z$$

with $\rho=28$, $\sigma=10$, $\beta=8/3$.  Setting up a simple Euler
scheme:

```{r setuplorenz}
rho <- 28
sigma <- 10
beta <- 8/3


lorenz <- function(x,y,z,n,dt=0.01,giveX=FALSE){
  if(giveX){xx <- rep(0,n)}
  for(i in seq_len(n)){
    xnew <- x + dt * sigma*(y-x)
    ynew <- y + dt * (x*(rho-z)-y)
    znew <- z + dt * (x*y-beta*z)
    x <- xnew
    y <- ynew
    z <- znew
    if(giveX){xx[i] <- x}
  }
  if(giveX){ return(xx)} else {return(x)}
}	
```

Above we have a simple forward Euler method for solving the Lorenz
butterfly equations.  We may plot the numerical solutions.

```{r lor100}
jj1 <- unlist(lorenz(1,1,1,3000,giveX=TRUE))
jj2 <- unlist(lorenz(1,1,1.001,3000,giveX=TRUE))
plot(jj1,type='l',col='red')
points(jj2,type='l',col='black')
```

Now, we ask "what is the derivative of the solution at the final
point, as a function of the start values for the parameter?  This is
absurdly easy with dual numbers; we run the simulation in exactly the
same way, but use $1+\epsilon$ as a start value:

```{r uselor2000,cache=TRUE}
lorenz(1,1,1+eps,2000)
```

We see that after 2000 timesteps the derivative is quite high, about
$3\times 10^{7}$, and after $10^4$ timesteps we have:

```{r uselor10000,cache=TRUE}
system.time(l10000 <- lorenz(1,1,1+eps,10000))
```

```{r}
l10000
```

Thus the gradient is about $5\times 10^{41}$: chaos writ large (NB:
this figure is subject to _extremely_ large inter-run variability).
We can demonstrate dynamical chaos somewhat more interestingly as
follows by perturbing the start point very slightly and observing the
result:

```{r uselor10000d,cache=TRUE}
oneplusdelta <- 1+1e-14   # slight perturbation
system.time(l10000d <- lorenz(oneplusdelta,1,1+eps,10000))
```

```{r}
l10000d
```

We see, as expected, that the value at $10^4$ timesteps is very
different, but also may observe that the gradient as calculated is
about $10^{45}$, three or four orders of magnitude greater than that
calculated in the the first trial above.  Of course, we could further
assess this by trying the same simulation but with many slightly
different starting values to get an idea of the distribution of the
gradient.

## Comparison with the `dual` package

The `dual` package is a bespoke system for working with dual numbers
and is much better than `clifford` for this purpose.  Here I show how
to reproduce some of the results from the `dual` README file:

```{r dual}
library("dual")
x <- dual(f = 1.5, grad = c(1,0,0))
y <- dual(f = 0.5, grad = c(0,1,0))
z <- dual(f = 1.0, grad = c(0,0,1))
exp(z - x) * sin(x)^y / x
```

To reproduce this with the `clifford` package we can use the
multivariate formalism directly:

```{r dualclifford}
f <- function(x,y,z){exp(z - x)*sin(x)^y / x}
x <- 1.5 ; y <- 0.5 ; z <- 1.0
L <- list(f(x+eps,y,z),f(x,y+eps,z),f(x,y,z+eps))
rbind(lapply(L,getcoeffs,0),lapply(L,getcoeffs,1))
```

[observe that $\left.\partial f/\partial z\right|_{x,y,z}=f(x,y,z)$].

It is technically possible to reproduce the nice vectorised
functionality available in the `dual` package with clifford objects of
the form $x+\alpha e_1 + \beta e_2+\gamma e_3$, but it is awkward and
inefficient.  The issue is the elimination of cross-terms in products,
and this can't be done in standard Clifford formalism (one needs to
redefine multiplication so that cross terms are discarded).


## References
